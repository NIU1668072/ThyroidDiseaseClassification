{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name= 'thyroidDF'\n",
    "df = pd.read_csv(f'data/{dataset_name}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.dtypes)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boolean_columns = df.columns[df.isin(['t', 'f']).all()]\n",
    "df[boolean_columns] = df[boolean_columns].applymap(lambda x: 1 if x == 't' else 0)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperthyroid = {'A', 'B', 'C', 'D'}\n",
    "hypothyroid = {'E', 'F', 'G', 'H'}\n",
    "\n",
    "def classify_diagnosis(diagnosis):\n",
    "    \n",
    "    letters = set(diagnosis.replace('|', ''))\n",
    "    if letters & hyperthyroid:\n",
    "        return 2\n",
    "    \n",
    "    if letters & hypothyroid:\n",
    "        return 0\n",
    "    \n",
    "    return 1\n",
    "\n",
    "df['target'] = df['target'].apply(classify_diagnosis)\n",
    "df['target'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['patient_id'],axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "mesures = ['TSH_measured', 'T3_measured','TT4_measured','T4U_measured','FTI_measured','TBG_measured']\n",
    "valors = ['TSH', 'T3','TT4','T4U','FTI', 'TBG']\n",
    "df_cleared = df.drop(mesures+valors,axis='columns')\n",
    "df_cleared = df_cleared.dropna()\n",
    "numeric_features = df_cleared.select_dtypes(np.number).keys()\n",
    "non_numeric = [k for k in df_cleared.keys() if k not in numeric_features]\n",
    "df_cleared.loc[:,non_numeric] = df_cleared.loc[:,non_numeric].apply(LabelEncoder().fit_transform)\n",
    "\n",
    "correlation_matrix = df_cleared.corr(method='pearson', min_periods=1, numeric_only=False)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', linewidths=0.5) \n",
    "plt.title('Heatmap de la matriu de correlació', fontsize=15)\n",
    "plt.ylabel('Variables', fontsize = 12)\n",
    "plt.savefig('correlation_matrix_atributs_no_mesures.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribució de la variable 'Survived'\n",
    "plt.figure(figsize=(4, 2))\n",
    "sns.countplot(x='target', data=df, palette='viridis')\n",
    "plt.title('Distribució de target')\n",
    "plt.xlabel('Survived (0 = No, 1 = Yes)')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(ticks=[1, 0, 2], labels=['healthy','hypothyroid','hyperthyroid'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pivot = df.groupby(['referral_source', 'target']).size().unstack(fill_value=0)\n",
    "\n",
    "data_pivot = data_pivot.div(data_pivot.sum(axis=1), axis=0)\n",
    "\n",
    "data_pivot.plot(kind='bar', stacked=True, colormap='viridis')\n",
    "plt.ylabel('Proporció')\n",
    "plt.title('Distribució de target per referral_source')\n",
    "plt.savefig(\"proporcio_referral_source_target.pdf\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_percentage = df.isna().mean()\n",
    "nan_columns = nan_percentage[nan_percentage > 0]\n",
    "nan_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['referral_source'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['referral_source'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies_referral_source = pd.get_dummies(df['referral_source'], dtype='int',drop_first=False)\n",
    "df = pd.concat([df, dummies_referral_source], axis=1)\n",
    "df = df.drop(['referral_source','WEST'], axis=\"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['age'].nsmallest(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['age'].nlargest(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['age'] < 100]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Omplir NaNs de 'sexe' amb 'F' si 'pregnant' és 1\n",
    "df.loc[(df['sex'].isna()) & (df['pregnant'] == 1), 'sex'] = 'F'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_nan = df['target'].value_counts(normalize=True).sort_index()\n",
    "df.dropna(subset=['sex'], inplace=True)\n",
    "dist_not_nan = df['target'].value_counts(normalize=True).sort_index()\n",
    "\n",
    "print(\"Distribució target amb NaNs:\")\n",
    "print(dist_nan)\n",
    "print(\"\\nDistribució target sense NaNs:\")\n",
    "print(dist_not_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies_sex = pd.get_dummies(df['sex'], dtype='int',drop_first=False)\n",
    "df = pd.concat([df, dummies_sex], axis=1)\n",
    "df = df.drop(['sex'], axis=\"columns\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_percentage = df.isna().mean()\n",
    "nan_columns = nan_percentage[nan_percentage > 0]\n",
    "nan_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesures = ['TSH_measured', 'T3_measured','TT4_measured','T4U_measured','FTI_measured','TBG_measured']\n",
    "valors = ['TSH', 'T3','TT4','T4U','FTI', 'TBG']\n",
    "for i in range(len(mesures)):\n",
    "    no_mesured = df[df[mesures[i]] == 0]\n",
    "    is_theory_correct = no_mesured[valors[i]].isna().all()\n",
    "    print(is_theory_correct)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(mesures,axis='columns',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fill_na = df.copy()\n",
    "valors_maxims_normals = [5.6,6,150,1.3,165,39]\n",
    "for i,j in zip(valors,valors_maxims_normals):\n",
    "    df_fill_na[i]/=j\n",
    "df_fill_na.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tornar a crear les gràfiques, aquesta vegada partint del dataframe\n",
    "fig, axes = plt.subplots(3, 2, figsize=(12, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, column in zip(axes, valors):\n",
    "    ax.hist(df_fill_na[column], bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    ax.set_title(f'Distribució de {column}')\n",
    "    ax.set_xlabel(column)\n",
    "    ax.set_ylabel('Freqüència')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(valors)):\n",
    "    print(df_fill_na[valors[i]].min(), df_fill_na[valors[i]].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in valors:\n",
    "    std_dev = df[i].std()  \n",
    "    df_fill_na[i] = df_fill_na[i].fillna(-1)\n",
    "df_fill_na.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Tornar a crear les gràfiques, aquesta vegada partint del dataframe\n",
    "fig, axes = plt.subplots(3, 2, figsize=(12, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, column in zip(axes, valors):\n",
    "    ax.hist(df_fill_na[column], bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    ax.set_title(f'Distribució de {column}')\n",
    "    ax.set_xlabel(column)\n",
    "    ax.set_ylabel('Freqüència')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fill_na['age'] /= 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fill_na.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_fill_na['target'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "df_cleared = df[valors+['target']]\n",
    "correlation_matrix = df_cleared.corr(method='pearson', min_periods=1, numeric_only=False)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix,annot=True, cmap='coolwarm',fmt='.2f', linewidths=0.5) \n",
    "plt.title('Heatmap de la matriu de correlació')\n",
    "plt.ylabel('Variables', fontsize = 12)\n",
    "plt.savefig('correlation_matrix_atributs_df_fillna.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear subplots\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(16, 8))\n",
    "\n",
    "# Filtrar dades per sexe\n",
    "women = df_fill_na[df_fill_na['M'] == 0]\n",
    "men = df_fill_na[df_fill_na['M'] == 1]\n",
    "\n",
    "# Colors i targets\n",
    "targets_numeric = [1,0,2]\n",
    "colors = ['green', 'red', 'blue']\n",
    "target_categorical=['healthy','hypothyroid','hyperthyroid']\n",
    "\n",
    "# Subplot per a dones\n",
    "for n, color,c in zip(targets_numeric, colors,target_categorical):\n",
    "    sns.histplot(\n",
    "        data=women[women['target'] == n],\n",
    "        x=\"age\",\n",
    "        bins=18,\n",
    "        label=c,\n",
    "        ax=axes[0],\n",
    "        color=color,\n",
    "        kde=False\n",
    "    )\n",
    "axes[0].legend()\n",
    "axes[0].set_title('Female')\n",
    "\n",
    "# Subplot per a homes\n",
    "for n, color,c in zip(targets_numeric, colors,target_categorical):\n",
    "    sns.histplot(\n",
    "        data=men[men['target'] == n],\n",
    "        x=\"age\",\n",
    "        bins=18,\n",
    "        label=c,\n",
    "        ax=axes[1],\n",
    "        color=color,\n",
    "        kde=False\n",
    "    )\n",
    "axes[1].legend()\n",
    "axes[1].set_title('Male')\n",
    "\n",
    "# Mostrar la gràfica\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"relacio_edat_sexe_target.pdf\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "target_att = 'target'\n",
    "attributes = [k for k in df.keys() if k!= target_att]\n",
    "X = df_fill_na[attributes]\n",
    "y = df_fill_na[[target_att]]\n",
    "X_train_val_fillna, X_test_fillna, y_train_val_fillna, y_test_fillna = train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_val_fillna.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_fillna.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# Provem PCA amb 2 components\n",
    "pca = PCA(n_components=2)\n",
    "y_pca = np.array(y_train_val_fillna)\n",
    "X_data_pca_2 = pca.fit_transform(X_train_val_fillna)\n",
    "\n",
    "fig = plt.figure(figsize = (8,8))\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
    "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
    "ax.set_title('2 component PCA', fontsize = 20)\n",
    "\n",
    "ax.scatter(X_data_pca_2[:,0], X_data_pca_2[:,1],c = y_pca[:], s = 40, cmap='viridis')\n",
    "ax.grid()\n",
    "plt.savefig(\"PCA_df_fillna.pdf\")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_data_pca_2, y_pca, test_size=0.2, random_state=20)\n",
    "model = GradientBoostingClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_val)\n",
    "y_pred_train = model.predict(X_train)\n",
    "cm = confusion_matrix(y_val, y_pred)\n",
    "\n",
    "print(f1_score(y_val,y_pred,average='macro'))\n",
    "print(accuracy_score(y_val,y_pred))\n",
    "print(\"Matriu de confusió:\\n\", cm)\n",
    "print(\"Train:\",f1_score(y_train,y_pred_train,average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val_fillna, y_train_val_fillna, test_size=0.2, random_state=0)\n",
    "model = GradientBoostingClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "cm = confusion_matrix(y_val, y_pred)\n",
    "\n",
    "print(f1_score(y_val,y_pred,average='macro'))\n",
    "print(accuracy_score(y_val,y_pred))\n",
    "print(\"Matriu de confusió:\\n\", cm)\n",
    "print(\"Train:\",f1_score(y_train,y_pred_train,average='macro'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classificacio_per_defecte(X, y, PRC, it, base_classifiers):\n",
    "    acc_r = np.zeros((it, len(base_classifiers)))\n",
    "\n",
    "    random_state = 20\n",
    "    for i in range(it):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=PRC, random_state=random_state * (i + 1))\n",
    "        for j, model in enumerate(base_classifiers.values()):\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            yhat = model.predict(X_test)\n",
    "            acc_r[i][j] = f1_score(y_test, yhat, average='macro')\n",
    "\n",
    "    return acc_r\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_df(acc_r,name_dataset,base_classifiers):\n",
    "    models = list(base_classifiers.keys())\n",
    "\n",
    "    data_for_saving = []\n",
    "    for j in range(acc_r.shape[1]):  \n",
    "        for i in range(acc_r.shape[0]):  \n",
    "            precisio = acc_r[i, j]\n",
    "            \n",
    "            data_for_saving.append({\n",
    "                'Model': models[j],\n",
    "                'Mostra': i + 1,  \n",
    "                'Precisio': precisio\n",
    "            })\n",
    "\n",
    "    df_results = pd.DataFrame(data_for_saving)\n",
    "\n",
    "    df_results.to_csv(f'./execucions/resultats_per_model_default_{name_dataset}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "random_state=20\n",
    "base_classifiers = {\n",
    "    'Logistic Regression OvO': OneVsOneClassifier(LogisticRegression(random_state=random_state)),\n",
    "    'Logistic Regression OvR': OneVsRestClassifier(LogisticRegression(random_state=random_state)),\n",
    "    'SVC OvO': OneVsOneClassifier(SVC(random_state=random_state)),\n",
    "    'SVC OvR': OneVsRestClassifier(SVC(random_state=random_state)),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=random_state),\n",
    "    'Random Forest': RandomForestClassifier(random_state=random_state),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=random_state),\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=5),\n",
    "    'XGBoost': XGBClassifier(random_state=random_state)\n",
    "}\n",
    "PRC = 0.3\n",
    "#f1_dfna = classificacio_per_defecte(X_train_val_fillna,y_train_val_fillna,PRC,10,base_classifiers)\n",
    "#save_df(f1_dfna,'df_fillna',base_classifiers)\n",
    "f1_dfna = pd.read_csv('./execucions/resultats_per_model_default_df_fillna.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(acc_r):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.boxplot(data=acc_r, x=\"Model\", y=\"Precisio\")\n",
    "    plt.title(\"Comparativa de f1_score per Model\")\n",
    "    plt.ylabel(\"f1_score\")\n",
    "    plt.xlabel(\"Model\")\n",
    "    plt.legend(title=\"Tipus\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(f1_dfna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "\n",
    "selected_data = {}\n",
    "\n",
    "def select_important_features(model, X_train, y_train, threshold='mean'):\n",
    "    model.fit(X_train, y_train)\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        selector = SelectFromModel(model, threshold=threshold, prefit=True)\n",
    "        mask = selector.get_support()\n",
    "    elif hasattr(model, 'coef_'):\n",
    "        selector = SelectFromModel(model, threshold=threshold, prefit=True)\n",
    "        mask = selector.get_support()\n",
    "    else:\n",
    "        print(f\" --- El model {model.__class__.__name__} no suporta selecció de característiques. S'afageixen totes les variables---\")\n",
    "        mask = np.ones(X_train.shape[1], dtype=bool)\n",
    "    X_norm_selected = X_train[:, mask] if isinstance(X_train, np.ndarray) else X_train.loc[:, mask]\n",
    "    return X_norm_selected, mask\n",
    "\n",
    "\n",
    "for name, model in base_classifiers.items():\n",
    "    print(f\"Seleccionant característiques importants per {name}\")\n",
    "    X_norm_selected, mask = select_important_features(model, X_train_val_fillna, y_train_val_fillna)\n",
    "    selected_data[f'{name}'] = X_norm_selected\n",
    "    selected_data[f'{name}_mask'] = mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "classifier_names = base_classifiers.keys()\n",
    "feature_selection_df = pd.DataFrame(\n",
    "    index=classifier_names, \n",
    "    columns=attributes\n",
    ")\n",
    "for model_name in classifier_names:\n",
    "    mask = selected_data[f'{model_name}_mask']\n",
    "    feature_selection_df.loc[model_name] = mask\n",
    "\n",
    "feature_selection_df = feature_selection_df.astype(int)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(feature_selection_df, cmap='Blues', annot=True, cbar=False, \n",
    "            linewidths=.5, linecolor='black')\n",
    "plt.title('Característiques Seleccionades per Model', fontsize=16)\n",
    "plt.xlabel('Característiques', fontsize=12)\n",
    "plt.ylabel('Models', fontsize=12)\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features_models = {\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=random_state),\n",
    "    'Random Forest': RandomForestClassifier(random_state=random_state),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=random_state),\n",
    "    'XGBoost': XGBClassifier(random_state=random_state)\n",
    "}\n",
    "it = 10\n",
    "acc_r = np.zeros((it, len(selected_features_models)))\n",
    "\n",
    "random_state = 20\n",
    "for i in range(it):\n",
    "    j=0\n",
    "    for name, model in selected_features_models.items():\n",
    "        X_train, X_test, y_train, y_test = train_test_split(selected_data[name], y_train_val_fillna, test_size=PRC, random_state=random_state * (i + 1))\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        yhat = model.predict(X_test)\n",
    "        acc_r[i][j] = f1_score(y_test, yhat, average='macro')\n",
    "        j += 1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_df(acc_r,'selected_features_df_fillna',selected_features_models)\n",
    "s_f = pd.read_csv('./execucions/resultats_per_model_default_selected_features_df_fillna.csv')\n",
    "plot_data(s_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paràmetres pel Logistic Regressor\n",
    "lr_param_grid = {\n",
    "    'estimator__random_state' : [random_state],  \n",
    "    'estimator__penalty' : ['l2', 'l1'], \n",
    "    'estimator__solver' : ['lbfgs', 'liblinear'],  \n",
    "    'estimator__C' : [0.01, 0.1, 1.0, 10.0, 100.0],\n",
    "}\n",
    "\n",
    "svc_param_grid = {'estimator__random_state' : [random_state],\n",
    "                  'estimator__C' : [0.5, 1.0, 2.0, 4.0],\n",
    "                  'estimator__gamma' : ['auto'],\n",
    "                  'estimator__kernel' : ['rbf', 'linear', 'poly', 'sigmoid'],\n",
    "                  'estimator__shrinking' : [True, False]}\n",
    "\n",
    "\n",
    "# Paràmetres pel DTC\n",
    "dtc_param_grid = {'random_state' : [random_state],\n",
    "                  'criterion' : ['gini'],\n",
    "                  'max_depth' : [None, 3, 5, 7, 10],\n",
    "                  'min_samples_leaf' : [1, 5, 10], \n",
    "                  'min_samples_split' : [2, 5, 10],\n",
    "                  'max_features': [None, 1, 2, 3, 4]}\n",
    "\n",
    "# Paràmetres pel RFC\n",
    "rfc_param_grid = {'random_state': [random_state],\n",
    "                  'criterion': ['gini', 'entropy'],\n",
    "                  'max_features': [None, 2, 3, 4],\n",
    "                  'max_depth': [None, 10, 20, 30],\n",
    "                  'min_samples_split': [2, 5, 10],\n",
    "                  'min_samples_leaf': [1, 2, 4],\n",
    "                  'class_weight': [None, 'balanced'],\n",
    "}\n",
    "\n",
    "# Paràmetres pel GBC\n",
    "gbc_param_grid = {\n",
    "    'random_state': [random_state],\n",
    "    'loss': ['log_loss'], \n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 5],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Paràmetres pel KNN\n",
    "knn_param_grid = {\n",
    "    'n_neighbors': [3, 5, 10, 15],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "    'leaf_size': [30, 40, 50],\n",
    "    'p': [1, 2],\n",
    "    'metric': ['euclidean', 'manhattan', 'chebyshev'],\n",
    "}\n",
    "\n",
    "# Paràmetres pel XGBoost\n",
    "xgb_param_grid = {\n",
    "    'random_state': [random_state],\n",
    "    'n_estimators': [100, 200, 300],  \n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],  \n",
    "    'max_depth': [3, 5, 7, 10], \n",
    "    'min_child_weight': [1, 3, 5],  \n",
    "    'subsample': [0.6, 0.8, 1.0],  \n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_list = {\n",
    "    'Logistic Regression OvO': lr_param_grid, \n",
    "    'Logistic Regression OvR': lr_param_grid, \n",
    "    'SVC OvO': svc_param_grid,\n",
    "    'SVC OvR': svc_param_grid,\n",
    "    'Decision Tree': dtc_param_grid, \n",
    "    'Random Forest': rfc_param_grid, \n",
    "    'Gradient Boosting': gbc_param_grid,\n",
    "    'KNN': knn_param_grid,\n",
    "    'XGBoost': xgb_param_grid\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "def cerca_hyperparametres(X,y,base_classifiers,splits,dataset_name,param_grid_list):\n",
    "    random_state = 42\n",
    "\n",
    "    kfold = StratifiedKFold(n_splits=splits, random_state=random_state, shuffle=True)\n",
    "\n",
    "    for name, model in base_classifiers.items():\n",
    "        results = []\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        param_grid = param_grid_list[name]\n",
    "\n",
    "        gs = GridSearchCV(estimator=model, param_grid=param_grid, cv=kfold, \n",
    "                            scoring=make_scorer(f1_score, average='macro'), n_jobs=-1)\n",
    "        gs.fit(X, y)\n",
    "        mean_f1_best_model = gs.best_score_\n",
    "        best_params = gs.best_params_\n",
    "\n",
    "\n",
    "        end_time = time.time()\n",
    "        calc_time = end_time - start_time\n",
    "\n",
    "        results.append({\n",
    "            'Classifier': name,\n",
    "            'Mean Test Score (F1)': mean_f1_best_model,\n",
    "            'Execution Time (s)': calc_time,  \n",
    "            'Best Parameters': best_params,\n",
    "            'Best Estimator': gs.best_estimator_  \n",
    "        })\n",
    "\n",
    "\n",
    "        print(f'Classifier: {type(model).__name__}, Mean Test Score: {mean_f1_best_model:.4f}, Time: {calc_time:.2f}s')\n",
    "\n",
    "        results_df = pd.DataFrame(results)\n",
    "        results_df.to_csv(f'./execucions/{name}_hyperparameters_search_results_{dataset_name}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_classifiers_df_fillna = {\n",
    "    'Logistic Regression OvO': OneVsOneClassifier(LogisticRegression(random_state=random_state)),\n",
    "    'Logistic Regression OvR': OneVsRestClassifier(LogisticRegression(random_state=random_state)),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=random_state),\n",
    "    'Random Forest': RandomForestClassifier(random_state=random_state),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=random_state),\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=5),\n",
    "    'XGBoost': XGBClassifier(random_state=random_state)\n",
    "}\n",
    "#cerca_hyperparametres(X_train_val_fillna,y_train_val_fillna,base_classifiers_df_fillna,10,'df_fillna',param_grid_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, model in selected_features_models.items():\n",
    "    cerca_hyperparametres(selected_data[name],y_train_val_fillna,{name: model},10,'selected_features_df_fillna',param_grid_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_model_cerca_hyperparametres(base_classifiers,dataset_name):\n",
    "    best_model=LogisticRegression()\n",
    "    f1=0\n",
    "    for name, model in base_classifiers.items():\n",
    "        l=pd.read_csv(f'./execucions/{name}_hyperparameters_search_results_{dataset_name}.csv')\n",
    "        l = l.loc[l['Mean Test Score (F1)'].idxmax()]\n",
    "        if f1 < l['Mean Test Score (F1)']:\n",
    "            f1 = l['Mean Test Score (F1)']\n",
    "            best_params = eval(l['Best Parameters'])\n",
    "            best_model = model\n",
    "            best_model.set_params(**best_params)\n",
    "    return best_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_dfna = get_best_model_cerca_hyperparametres(base_classifiers_df_fillna,'df_fillna')\n",
    "best_model_dfna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def passar_a_intervals(df,hormones,intervals):\n",
    "    def classificar_h(h,interval):\n",
    "        if pd.isna(h):  \n",
    "            return [0, 0, 0]\n",
    "        elif h < interval[0]:\n",
    "            return [1, 0, 0]\n",
    "        elif h < interval[1]:\n",
    "            return [0, 1, 0]\n",
    "        else:\n",
    "            return [0, 0, 1]\n",
    "\n",
    "\n",
    "    for hormona,interval in zip(hormones,intervals):\n",
    "        df[[f'{hormona} Baix', f'{hormona} Normal', f'{hormona} Alt']] = df[hormona].apply(lambda x: pd.Series(classificar_h(x,interval)))\n",
    "        df.drop([hormona],axis='columns',inplace=True)\n",
    "    return df\n",
    "\n",
    "intervals=[[0.26,5.6],\n",
    "            [1.8,4.6],\n",
    "            [60,150],\n",
    "            [0.7,1.3],\n",
    "            [60,165],\n",
    "            [13,39]\n",
    "]\n",
    "df_intervals = passar_a_intervals(df.copy(),valors,intervals)\n",
    "\n",
    "df_intervals.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bins = [0,20,35,50,65,80,100]\n",
    "labels=['0-19','20-34','35-49','50-64','65-79','80-99']\n",
    "df_intervals['Grup_Edat'] = pd.cut(df_intervals['age'], bins=bins, labels=labels, right=False)\n",
    "dummies_sex = pd.get_dummies(df_intervals['Grup_Edat'], dtype='int',drop_first=True)\n",
    "df_intervals = pd.concat([df_intervals, dummies_sex], axis=1)\n",
    "\n",
    "df_intervals = df_intervals.drop(['age','Grup_Edat'], axis=\"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars = [[f'{i} Alt',f'{i} Baix',f'{i} Normal'] for i in valors]\n",
    "vars = [j for a in vars for j in a]\n",
    "df_cleared = df_intervals[vars+['target']+labels[1:]]\n",
    "correlation_matrix = df_cleared.corr(method='pearson', min_periods=1, numeric_only=False)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', linewidths=0.5) \n",
    "plt.title('Heatmap de la matriu de correlació')\n",
    "plt.savefig('correlation_matrix_atributs_df_intervals.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = [k for k in df_intervals.keys() if k!= target_att]\n",
    "X = df_intervals[attributes]\n",
    "y = df_intervals[[target_att]]\n",
    "X_train_val_intervals, X_test_intervals, y_train_val_intervals, y_test_intervals = train_test_split(X, y, test_size=0.1, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "y_pca = np.array(y_train_val_intervals)\n",
    "X_data_pca_2 = pca.fit_transform(X_train_val_intervals)\n",
    "\n",
    "fig = plt.figure(figsize = (8,8))\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
    "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
    "ax.set_title('2 component PCA', fontsize = 20)\n",
    "\n",
    "ax.scatter(X_data_pca_2[:,0], X_data_pca_2[:,1],c = y_pca[:], s = 40, cmap='viridis')\n",
    "ax.grid()\n",
    "plt.savefig(\"PCA_df_intervals.pdf\")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_data_pca_2, y_train_val_intervals, test_size=0.2, random_state=20)\n",
    "model = GradientBoostingClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_val)\n",
    "y_pred_train = model.predict(X_train)\n",
    "cm = confusion_matrix(y_val, y_pred)\n",
    "\n",
    "print(f1_score(y_val,y_pred,average='macro'))\n",
    "print(accuracy_score(y_val,y_pred))\n",
    "print(\"Matriu de confusió:\\n\", cm)\n",
    "print(\"Train:\",f1_score(y_train,y_pred_train,average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val_intervals, y_train_val_intervals, test_size=0.2, random_state=20)\n",
    "model = GradientBoostingClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_val)\n",
    "y_pred_train = model.predict(X_train)\n",
    "cm = confusion_matrix(y_val, y_pred)\n",
    "\n",
    "print(f1_score(y_val,y_pred,average='macro'))\n",
    "print(accuracy_score(y_val,y_pred))\n",
    "print(\"Matriu de confusió:\\n\", cm)\n",
    "print(\"Train:\",f1_score(y_train,y_pred_train,average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRC = 0.3\n",
    "#f1_intervals = classificacio_per_defecte(X_train_val_intervals,y_train_val_intervals,PRC,10,base_classifiers)\n",
    "#save_df(f1_intervals,'df_interval',base_classifiers)\n",
    "f1_intervals = pd.read_csv('./execucions/resultats_per_model_default_df_interval.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(f1_intervals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cerca_hyperparametres(X_train_val_intervals,y_train_val_intervals,base_classifiers,10,'df_intervals',param_grid_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Inicialitzem els diccionaris per a gestionar els valors\n",
    "scores = {\n",
    "    \"df_fillna\": {},\n",
    "    \"df_intervals\": {},\n",
    "    \"selected_features_df_fillna\": {},\n",
    "    \"selected_features_df_intervals\": {}\n",
    "}\n",
    "\n",
    "# Llista dels tipus de fitxers\n",
    "file_categories = [\n",
    "    (\"df_fillna\", \"_hyperparameters_search_results_df_fillna.csv\"),\n",
    "    (\"df_intervals\", \"_hyperparameters_search_results_df_intervals.csv\"),\n",
    "    (\"selected_features_df_fillna\", \"_hyperparameters_search_results_selected_features_df_fillna.csv\"),\n",
    "    (\"selected_features_df_intervals\", \"_hyperparameters_search_results_selected_features_df_intervals.csv\")\n",
    "]\n",
    "\n",
    "# Llegim els fitxers disponibles i extreiem els valors de 'Mean Test Score (F1)'\n",
    "for model_name in base_classifiers.keys():\n",
    "    # Iterem sobre les categories de fitxers\n",
    "    for category, suffix in file_categories:\n",
    "        file_path = f\"execucions/{model_name}{suffix}\"\n",
    "\n",
    "        # Comprovar si el fitxer existeix i llegir la columna 'Mean Test Score (F1)'\n",
    "        if os.path.exists(file_path):\n",
    "            df = pd.read_csv(file_path)\n",
    "            scores[category][model_name] = df['Mean Test Score (F1)'].iloc[0]\n",
    "        else:\n",
    "            scores[category][model_name] = None\n",
    "\n",
    "# Filtrar models amb almenys un valor vàlid en qualsevol dels fitxers\n",
    "valid_models = [\n",
    "    model for model in base_classifiers.keys()\n",
    "    if any(scores[category].get(model) is not None for category in scores)\n",
    "]\n",
    "\n",
    "# Preparar valors per a la gràfica\n",
    "x = range(len(valid_models))\n",
    "values = {category: [scores[category].get(model, 0) if scores[category].get(model) is not None else 0 for model in valid_models]\n",
    "          for category in scores}\n",
    "\n",
    "# Configuració per a la gràfica\n",
    "bar_width = 0.2\n",
    "offsets = [-1.5 * bar_width, -0.5 * bar_width, 0.5 * bar_width, 1.5 * bar_width]\n",
    "colors = ['skyblue', 'orange', 'green', 'purple']\n",
    "labels = [\n",
    "    'df_fillna',\n",
    "    'df_intervals',\n",
    "    'selected_features_df_fillna',\n",
    "    'selected_features_df_intervals'\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Dibuixem les barres per a cada categoria\n",
    "for i, (category, vals) in enumerate(values.items()):\n",
    "    plt.bar([pos + offsets[i] for pos in x], vals, bar_width, label=labels[i], color=colors[i])\n",
    "\n",
    "# Configuració dels eixos i estètica\n",
    "plt.xlabel('Model', fontsize=12)\n",
    "plt.ylabel('Mean Test Score (F1)', fontsize=12)\n",
    "plt.title('Comparació de Mean Test Score (F1) per models i tipus de fitxer', fontsize=14)\n",
    "plt.xticks(x, valid_models, rotation=45, ha='right')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, class_names=None, figsize=(8, 6)):\n",
    "    # Calcular la matriu de confusió\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Crear una matriu de colors\n",
    "    cm_colored = np.full_like(cm, fill_value=0.5, dtype=float)  # Valor per defecte blanc (1.0)\n",
    "    \n",
    "    # Assignar colors: Verd per la diagonal i Vermell per la resta\n",
    "    for i in range(len(cm)):\n",
    "        for j in range(len(cm)):\n",
    "            if cm[i, j] != 0:\n",
    "                if i == j:\n",
    "                    cm_colored[i, j] = 0.7  # Verd per la diagonal\n",
    "                else:\n",
    "                    cm_colored[i, j] = 0.1  # Vermell per la resta\n",
    "\n",
    "    # Crear la imatge de la matriu de confusió\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.imshow(cm_colored, cmap='RdYlGn', interpolation='nearest', vmin=0, vmax=1)\n",
    "    \n",
    "    # Afegir els valors sobre la matriu\n",
    "    for i in range(len(cm)):\n",
    "        for j in range(len(cm)):\n",
    "            plt.text(j, i, cm[i, j], ha='center', va='center', \n",
    "                     color='black' if cm[i, j] != 0 else 'white', fontsize=12)\n",
    "    \n",
    "    # Afegir títols i etiquetes\n",
    "    plt.title(\"Matriu de Confusió\", fontsize=15)\n",
    "    plt.xlabel('Etiqueta Predicha', fontsize=12)\n",
    "    plt.ylabel('Etiqueta Real', fontsize=12)\n",
    "    \n",
    "    # Ajustar les etiquetes dels eixos si es donen\n",
    "    plt.xticks(np.arange(len(class_names)), class_names, rotation=45)\n",
    "    plt.yticks(np.arange(len(class_names)), class_names)\n",
    "\n",
    "    # Mostrar la matriu\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_dfna.fit(X_train_val_fillna,y_train_val_fillna)\n",
    "y_pred = best_model_dfna.predict(X_test_fillna)\n",
    "plot_confusion_matrix(y_test_fillna, y_pred,['hipotiroidisme','sa','hipertiroidisme'])\n",
    "\n",
    "print(f1_score(y_test_fillna,y_pred,average='macro'))\n",
    "print(f1_score(y_test_fillna,y_pred,average='micro'))\n",
    "print(accuracy_score(y_test_fillna,y_pred))\n",
    "print(\"Matriu de confusió:\\n\", cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_intervals = get_best_model_cerca_hyperparametres(base_classifiers,'df_intervals')\n",
    "best_model_intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_intervals.fit(X_train_val_intervals,y_train_val_intervals)\n",
    "y_pred = best_model_intervals.predict(X_test_intervals)\n",
    "\n",
    "cm = confusion_matrix(y_test_intervals, y_pred)\n",
    "\n",
    "print(f1_score(y_test_intervals,y_pred,average='macro'))\n",
    "print(f1_score(y_test_intervals,y_pred,average='micro'))\n",
    "print(accuracy_score(y_test_intervals,y_pred))\n",
    "print(\"Matriu de confusió:\\n\", cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "\n",
    "selected_data = {}\n",
    "\n",
    "def select_important_features(model, X_train, y_train, threshold='mean'):\n",
    "    model.fit(X_train, y_train)\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        selector = SelectFromModel(model, threshold=threshold, prefit=True)\n",
    "        mask = selector.get_support()\n",
    "    elif hasattr(model, 'coef_'):\n",
    "        selector = SelectFromModel(model, threshold=threshold, prefit=True)\n",
    "        mask = selector.get_support()\n",
    "    else:\n",
    "        print(f\" --- El model {model.__class__.__name__} no suporta selecció de característiques. S'afageixen totes les variables---\")\n",
    "        mask = np.ones(X_train.shape[1], dtype=bool)\n",
    "    X_norm_selected = X_train[:, mask] if isinstance(X_train, np.ndarray) else X_train.loc[:, mask]\n",
    "    return X_norm_selected, mask\n",
    "\n",
    "\n",
    "for name, model in base_classifiers.items():\n",
    "    print(f\"Seleccionant característiques importants per {name}\")\n",
    "    X_norm_selected, mask = select_important_features(model, X_train_val_intervals, y_train_val_intervals)\n",
    "    selected_data[f'{name}'] = X_norm_selected\n",
    "    selected_data[f'{name}_mask'] = mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "classifier_names = base_classifiers.keys()\n",
    "feature_selection_df = pd.DataFrame(\n",
    "    index=classifier_names, \n",
    "    columns=attributes\n",
    ")\n",
    "for model_name in classifier_names:\n",
    "    mask = selected_data[f'{model_name}_mask']\n",
    "    feature_selection_df.loc[model_name] = mask\n",
    "\n",
    "feature_selection_df = feature_selection_df.astype(int)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(feature_selection_df, cmap='Blues', annot=True, cbar=False, \n",
    "            linewidths=.5, linecolor='black')\n",
    "plt.title('Característiques Seleccionades per Model', fontsize=16)\n",
    "plt.xlabel('Característiques', fontsize=12)\n",
    "plt.ylabel('Models', fontsize=12)\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features_models = {\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=random_state),\n",
    "    'Random Forest': RandomForestClassifier(random_state=random_state),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=random_state),\n",
    "    'XGBoost': XGBClassifier(random_state=random_state)\n",
    "}\n",
    "it = 10\n",
    "acc_r = np.zeros((it, len(selected_features_models)))\n",
    "\n",
    "random_state = 20\n",
    "for i in range(it):\n",
    "    j=0\n",
    "    for name, model in selected_features_models.items():\n",
    "        X_train, X_test, y_train, y_test = train_test_split(selected_data[name], y_train_val_intervals, test_size=PRC, random_state=random_state * (i + 1))\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        yhat = model.predict(X_test)\n",
    "        acc_r[i][j] = f1_score(y_test, yhat, average='macro')\n",
    "        j += 1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_df(acc_r,'selected_features_df_intervals',selected_features_models)\n",
    "s_f = pd.read_csv('./execucions/resultats_per_model_default_selected_features_df_intervals.csv')\n",
    "plot_data(s_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features_models = {\n",
    "    'XGBoost': XGBClassifier(random_state=random_state)\n",
    "}\n",
    "#for name, model in selected_features_models.items():\n",
    "    #cerca_hyperparametres(selected_data[name],y_train_val_intervals,{name: model},10,'selected_features_df_intervals',param_grid_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
